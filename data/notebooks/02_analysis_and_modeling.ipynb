# Running Performance Analysis & Modeling
# Master’s Thesis – Magdalena Vranić
#
# This notebook focuses on data cleaning, feature engineering,
# machine learning modeling, and evaluation for running performance prediction.
# Data collection is handled separately.

# ============================================================
# 1. IMPORTS
# ============================================================

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import scipy.stats as stats
import statsmodels.api as sm
from statsmodels.stats.diagnostic import het_breuschpagan


# ============================================================
# 2. LOAD DATA
# ============================================================

# NOTE:
# Raw data is not included in the repository due to privacy.
# Replace the path below with your local dataset if reproducing.

df = pd.read_csv("data/strava_running_data.csv")

print("Dataset shape:", df.shape)
display(df.head())


# ============================================================
# 3. INITIAL DATA INSPECTION
# ============================================================

df.info()
print("\nMissing values:\n")
print(df.isnull().sum())


# ============================================================
# 4. DATA CLEANING
# ============================================================

# Drop irrelevant or non-informative columns if present
df = df.drop(
    columns=[
        'resource_state', 'athlete', 'name', 'id', 'map',
        'gear_id', 'upload_id', 'upload_id_str', 'external_id'
    ],
    errors='ignore'
)

# Remove duplicates
df = df.drop_duplicates()

print("After cleaning:", df.shape)


# ============================================================
# 5. FEATURE ENGINEERING
# ============================================================

# Unit conversions
if 'distance' in df.columns:
    df['distance_km'] = df['distance'] / 1000

if 'moving_time' in df.columns:
    df['duration_min'] = df['moving_time'] / 60

# Convert numeric columns safely
numeric_cols = [
    'average_heartrate', 'max_heartrate', 'suffer_score',
    'average_cadence', 'average_speed', 'max_speed',
    'elev_high', 'elev_low', 'distance', 'total_elevation_gain'
]

for col in numeric_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')


# ============================================================
# 6. HANDLE MISSING VALUES
# ============================================================

df_model = df.copy()

fill_mean_cols = [
    'average_cadence', 'average_heartrate',
    'max_heartrate', 'elev_high', 'elev_low', 'suffer_score'
]

for col in fill_mean_cols:
    if col in df_model.columns:
        df_model[col] = df_model[col].fillna(df_model[col].mean())


# ============================================================
# 7. FEATURE SELECTION
# ============================================================

features = [
    'distance', 'moving_time', 'elapsed_time',
    'total_elevation_gain', 'max_speed',
    'average_cadence', 'has_heartrate',
    'average_heartrate', 'max_heartrate',
    'elev_high', 'elev_low', 'pr_count', 'suffer_score'
]

features = [f for f in features if f in df_model.columns]

X = df_model[features]
y = df_model['average_speed']

print("Features used:", features)


# ============================================================
# 8. TRAIN / TEST SPLIT
# ============================================================

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)


# ============================================================
# 9. BASELINE MODEL
# ============================================================

baseline_model = GradientBoostingRegressor(random_state=42)
baseline_model.fit(X_train, y_train)

y_train_pred = baseline_model.predict(X_train)
y_test_pred = baseline_model.predict(X_test)


def evaluate(y_true, y_pred, label):
    print(f"\n--- {label} ---")
    print("MAE:", mean_absolute_error(y_true, y_pred))
    print("RMSE:", np.sqrt(mean_squared_error(y_true, y_pred)))
    print("R²:", r2_score(y_true, y_pred))


evaluate(y_train, y_train_pred, "Baseline – Train")
evaluate(y_test, y_test_pred, "Baseline – Test")


# ============================================================
# 10. HYPERPARAMETER TUNING
# ============================================================

param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.05, 0.1],
    'max_depth': [3, 4],
    'subsample': [0.8, 1.0]
}

grid_search = GridSearchCV(
    estimator=GradientBoostingRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)


# ============================================================
# 11. FINAL MODEL
# ============================================================

best_model = grid_search.best_estimator_

y_test_pred = best_model.predict(X_test)

evaluate(y_test, y_test_pred, "Optimized Model – Test")


# ============================================================
# 12. FEATURE IMPORTANCE
# ============================================================

importances = best_model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.bar(range(len(importances)), importances[indices])
plt.xticks(range(len(importances)), X_train.columns[indices], rotation=45)
plt.title("Feature Importance")
plt.tight_layout()
plt.show()


# ============================================================
# 13. RESIDUAL ANALYSIS
# ============================================================

residuals = y_test - y_test_pred

plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test_pred, y=residuals)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel("Predicted values")
plt.ylabel("Residuals")
plt.title("Residual Analysis – Test Set")
plt.show()


# ============================================================
# 14. STATISTICAL DIAGNOSTICS
# ============================================================

# Shapiro-Wilk test for normality
shapiro_stat, shapiro_p = stats.shapiro(residuals)
print("Shapiro-Wilk p-value:", shapiro_p)

# Breusch-Pagan test for heteroskedasticity
bp_test = het_breuschpagan(residuals, sm.add_constant(y_test_pred))
print("Breusch-Pagan p-value:", bp_test[1])


# ============================================================
# 15. ACTUAL VS PREDICTED
# ============================================================

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_test_pred, alpha=0.6)
plt.plot(
    [y_test.min(), y_test.max()],
    [y_test.min(), y_test.max()],
    'r--'
)
plt.xlabel("Actual values")
plt.ylabel("Predicted values")
plt.title("Actual vs Predicted – Test Set")
plt.show()
